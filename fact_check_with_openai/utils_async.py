import os, traceback, json
import asyncio
import re
from typing import List, Dict
from dotenv import load_dotenv
from openai import AsyncOpenAI
from datetime import datetime
import aiohttp

load_dotenv()

def translate_date_references(text: str) -> str:
    """
    ÿ•ÿ±ÿ¨ÿßÿπ ÿßŸÑŸÜÿµ ŸÉŸÖÿß ŸáŸà ÿØŸàŸÜ ÿ™ÿ∫ŸäŸäÿ± ÿßŸÑŸÖÿ±ÿßÿ¨ÿπ ÿßŸÑÿ≤ŸÖŸÜŸäÿ©
    ŸÑÿ™ÿ¨ŸÜÿ® ÿ™ÿ∫ŸäŸäÿ± ŸÖÿπŸÜŸâ ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜÿØ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÉŸÑŸÖÿßÿ™ ŸÖÿ´ŸÑ "ÿßŸÑŸäŸàŸÖ"
    """
    # ÿ•ÿ±ÿ¨ÿßÿπ ÿßŸÑŸÜÿµ ŸÉŸÖÿß ŸáŸà ÿØŸàŸÜ ÿ£Ÿä ÿ™ÿπÿØŸäŸÑ
    return text

async def generate_professional_news_article_from_analysis_async(claim_text: str, case: str, talk: str, sources: List[Dict], lang: str = "ar", client: AsyncOpenAI = None) -> str:
    """
    Generate a professional news article based on fact-check analysis and sources
    Uses the analysis (talk) and sources to create a balanced, journalistic piece
    """
    
    # Prepare sources context
    if not sources:
        sources_context = "No specific sources available for this investigation."
    else:
        sources_context = "\n\n".join([
            f"**Source {i+1}:**\n"
            f"Title: {source.get('title', 'N/A')}\n"
            f"URL: {source.get('url', 'N/A')}\n"
            f"Snippet: {source.get('snippet', 'N/A')}"
            for i, source in enumerate(sources[:5])  # Limit to 5 sources
        ])
    
    # Determine the prompt based on the case
    if case.lower() in {"ÿ≠ŸÇŸäŸÇŸä", "true", "vrai", "verdadero", "pravda"}:
        # TRUE case - Use the specific prompt for confirmed news
        FACT_CHECK_NEWS_PROMPT = f"""
You are a senior international news agency journalist writing in {lang.upper()} language.

Write a professional news article in the style of international news agencies based on the provided headline and analysis.

**MANDATORY REQUIREMENT:**
- You MUST write about the headline and analysis provided in the user message
- Extract ALL facts and details from the Fact-check Analysis provided by the user
- Do NOT create unrelated news - only use information from the provided analysis
- The headline is: "{claim_text}"
- Use the analysis to write the news article about this specific headline

**CRITICAL INSTRUCTIONS FOR TRUE NEWS:**
- Start DIRECTLY with the news event/statement itself (e.g., "ÿ£ÿ±ÿ≥ŸÑÿ™ [ÿßŸÑÿØŸàŸÑÿ©/ÿßŸÑŸáŸäÿ¶ÿ©]..." or "[Entity] sent...")
- Write as a DIRECT NEWS REPORT, NOT as analysis or verification
- First paragraph: Report the main event naturally with details (who, what, when, where, participants, etc.) based on the provided analysis
- Second paragraph: Discuss the topics, themes, or issues that were addressed/covered, using details from the analysis
- Third paragraph: Provide additional context about sessions, discussions, or highlights from the analysis
- AVOID any mention of "verification", "fact-check", "results", "ÿ™ÿ≠ŸÇŸÇ", "ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑÿ™ÿ≠ŸÇŸÇ" anywhere in the article
- Write naturally and smoothly as if reporting events as they happened
- Mention official sources and statements naturally from the analysis provided

**STRUCTURE TEMPLATE FOR TRUE NEWS:**
1. **Opening Paragraph**: Start directly with the event from the headline (e.g., "ÿ£ÿ±ÿ≥ŸÑÿ™ [ÿßŸÑÿØŸàŸÑÿ©]..." or "[Entity] sent...") with key details from the analysis
2. **Second Paragraph**: Discuss the details, quantities, beneficiaries, or specific information from the analysis
3. **Third Paragraph**: Additional context about significance, continuation, or broader implications from the analysis

**REQUIREMENTS:**
- Language: {lang.upper()} entirely
- Style: Professional news reporting (like AFP, Reuters, AP)
- Tone: Neutral, factual, authoritative
- Structure: Exactly 3 paragraphs following the template above
- Length: 150-250 words
- Must follow the exact structure template
- Use professional journalistic language
- NO mention of verification or fact-checking
"""
    else:
        # UNCERTAIN case - Use the specific prompt for unconfirmed news
        FACT_CHECK_NEWS_PROMPT = f"""
You are a senior international news agency journalist writing in {lang.upper()} language.

Write a professional news article in the style of international news agencies based on the provided headline and analysis.

**CRITICAL INSTRUCTIONS FOR UNCERTAIN NEWS:**
- Start with: "ÿ™ÿØÿßŸàŸÑÿ™ ŸÖŸÜÿµÿßÿ™ ÿßŸÑÿ™ŸàÿßÿµŸÑ ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπŸä ŸÖÿ≤ÿßÿπŸÖ ÿ™ŸÅŸäÿØ ÿ®ÿ£ŸÜ [ÿßŸÑÿßÿØÿπÿßÿ°]" (or equivalent in the target language)
- Follow immediately with: "ÿ∫Ÿäÿ± ÿ£ŸÜ ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑÿ™ÿ≠ŸÇŸÇ ÿ£ÿ∏Ÿáÿ±ÿ™ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿßÿØÿπÿßÿ° ŸÑÿß ŸäŸÖŸÉŸÜ ÿ™ÿ£ŸÉŸäÿØŸá" (or equivalent: "However, verification results showed that this claim cannot be confirmed")
- Then explain the available information and why the claim cannot be confirmed
- Provide historical context or relevant background information if available
- End with a clear conclusion that the claim lacks reliable evidence

**STRUCTURE TEMPLATE:**
1. **Opening**: "ÿ™ÿØÿßŸàŸÑÿ™ ŸÖŸÜÿµÿßÿ™ ÿßŸÑÿ™ŸàÿßÿµŸÑ ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπŸä ŸÖÿ≤ÿßÿπŸÖ ÿ™ŸÅŸäÿØ ÿ®ÿ£ŸÜ [ÿßŸÑÿßÿØÿπÿßÿ°]ÿå ÿ∫Ÿäÿ± ÿ£ŸÜ ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑÿ™ÿ≠ŸÇŸÇ ÿ£ÿ∏Ÿáÿ±ÿ™ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿßÿØÿπÿßÿ° ŸÑÿß ŸäŸÖŸÉŸÜ ÿ™ÿ£ŸÉŸäÿØŸá."
2. **Body**: Explain available information, historical context, and evidence that contradicts or doesn't support the claim
3. **Conclusion**: "Ÿàÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿ∞ŸÑŸÉÿå Ÿäÿ™ÿ®ŸäŸëŸÜ ÿ£ŸÜ ÿßŸÑÿßÿØÿπÿßÿ° ÿßŸÑŸÖÿ™ÿØÿßŸàŸÑ ŸäŸÅÿ™ŸÇÿ± ÿ•ŸÑŸâ ÿ£Ÿä ÿ£ÿ≥ÿßÿ≥ ŸÖŸÜ ÿßŸÑÿ£ÿØŸÑÿ© ÿßŸÑŸÖŸàÿ´ŸàŸÇÿ©ÿå ŸàŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÖÿµÿßÿØÿ± ÿ™ÿØÿπŸÖ ÿµÿ≠ÿ™Ÿá."

**REQUIREMENTS:**
- Language: {lang.upper()} entirely
- Style: Professional news reporting
- Tone: Objective, transparent, informative
- Structure: News article format with structured paragraphs
- Length: 150-250 words
- Must follow the exact structure template above
- Use professional journalistic language
"""
    
    # Create the user message
    if case.lower() in {"ÿ≠ŸÇŸäŸÇŸä", "true", "vrai", "verdadero", "pravda"}:
        user_message = f"""
**PROVIDED DATA:**
Headline: {claim_text}
Fact-check Analysis: {talk}

**AVAILABLE SOURCES:**
{sources_context}

**EXAMPLE FORMAT FOR TRUE NEWS (ARABIC):**
ÿ£ÿ±ÿ≥ŸÑÿ™ ÿØŸàŸÑÿ© ŸÇÿ∑ÿ± ŸÖÿ≥ÿßÿπÿØÿßÿ™ ÿ•ÿ∫ÿßÿ´Ÿäÿ© Ÿàÿ•ŸÜÿ≥ÿßŸÜŸäÿ© ÿπÿßÿ¨ŸÑÿ© ÿ•ŸÑŸâ ŸÖÿØŸäŸÜÿ© ÿßŸÑÿØÿ®ÿ© ŸÅŸä ÿßŸÑŸàŸÑÿßŸäÿ© ÿßŸÑÿ¥ŸÖÿßŸÑŸäÿ© ÿ®ÿ¨ŸÖŸáŸàÿ±Ÿäÿ© ÿßŸÑÿ≥ŸàÿØÿßŸÜÿå ŸÅŸä ÿ•ÿ∑ÿßÿ± ÿßŸÑÿ™ÿ≤ÿßŸÖŸáÿß ÿßŸÑÿ´ÿßÿ®ÿ™ ÿ®ÿØÿπŸÖ ÿßŸÑÿ¥ÿπÿ® ÿßŸÑÿ≥ŸàÿØÿßŸÜŸäÿå ŸÑÿß ÿ≥ŸäŸÖÿß ŸÅŸä ÿ∏ŸÑ ÿßŸÑÿ∏ÿ±ŸàŸÅ ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜŸäÿ© ÿßŸÑÿµÿπÿ®ÿ© ÿßŸÑÿ™Ÿä ŸäÿπŸäÿ¥Ÿáÿß ÿßŸÑŸÖÿØŸÜŸäŸàŸÜ ŸÖŸÜ ŸÜŸÇÿµ ÿ≠ÿßÿØ ŸÅŸä ÿßŸÑÿ∫ÿ∞ÿßÿ° Ÿàÿßÿ≠ÿ™Ÿäÿßÿ¨ ŸÖÿ™ÿ≤ÿßŸäÿØ ŸÑŸÖÿ≥ÿ™ŸÑÿ≤ŸÖÿßÿ™ ÿßŸÑÿ•ŸäŸàÿßÿ° ŸàÿßŸÑŸÖŸàÿßÿØ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©.

Ÿàÿ™ÿ¥ŸÖŸÑ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿßÿ™ ŸÜÿ≠Ÿà 3 ÿ¢ŸÑÿßŸÅ ÿ≥ŸÑÿ© ÿ∫ÿ∞ÿßÿ¶Ÿäÿ© Ÿà1650 ÿÆŸäŸÖÿ© ÿ•ŸäŸàÿßÿ° ŸàŸÖÿ≥ÿ™ŸÑÿ≤ŸÖÿßÿ™ ÿ£ÿÆÿ±Ÿâÿå ŸÖŸÇÿØŸÖÿ© ŸÖŸÜ ÿµŸÜÿØŸàŸÇ ŸÇÿ∑ÿ± ŸÑŸÑÿ™ŸÜŸÖŸäÿ© ŸàŸÇÿ∑ÿ± ÿßŸÑÿÆŸäÿ±Ÿäÿ©ÿå ŸÑÿØÿπŸÖ ÿßŸÑŸÜÿßÿ≤ÿ≠ŸäŸÜ ŸÖŸÜ ŸÖÿØŸäŸÜÿ© ÿßŸÑŸÅÿßÿ¥ÿ± ŸàÿßŸÑŸÖŸÜÿßÿ∑ŸÇ ÿßŸÑŸÖÿ¨ÿßŸàÿ±ÿ©ÿå ŸàŸÖŸÜ ÿßŸÑŸÖŸÇÿ±ÿ± ÿ£ŸÜ Ÿäÿ≥ÿ™ŸÅŸäÿØ ŸÖŸÜŸáÿß ÿ£ŸÉÿ´ÿ± ŸÖŸÜ 50 ÿ£ŸÑŸÅ ÿ¥ÿÆÿµÿå ŸÅÿ∂ŸÑÿß ÿπŸÜ ÿ•ŸÜÿ¥ÿßÿ° ŸÖÿÆŸäŸÖ ÿÆÿßÿµ ÿ®ÿßŸÑŸÖÿ≥ÿßÿπÿØÿßÿ™ ÿßŸÑŸÇÿ∑ÿ±Ÿäÿ© ÿ™ÿ≠ÿ™ ŸÖÿ≥ŸÖŸâ ŸÇÿ∑ÿ± ÿßŸÑÿÆŸäÿ±.

ŸàŸäÿπÿØ Ÿáÿ∞ÿß ÿßŸÑÿØÿπŸÖ ÿßŸÖÿ™ÿØÿßÿØÿß ŸÑÿ¨ŸáŸàÿØ ÿØŸàŸÑÿ© ŸÇÿ∑ÿ± ÿßŸÑŸÖÿ™ŸàÿßÿµŸÑÿ© ŸÅŸä ÿßŸÑŸàŸÇŸàŸÅ ÿ•ŸÑŸâ ÿ¨ÿßŸÜÿ® ÿßŸÑÿ¥ÿπÿ® ÿßŸÑÿ≥ŸàÿØÿßŸÜŸä ÿßŸÑÿ¥ŸÇŸäŸÇ Ÿàÿ™ÿÆŸÅŸäŸÅ ŸÖÿπÿßŸÜÿßÿ™Ÿá ÿ¨ÿ±ÿßÿ° ÿßŸÑŸÜÿ≤ÿßÿπ ÿßŸÑŸÖÿ≥ŸÑÿ≠ÿå ŸÉŸÖÿß Ÿäÿ¨ÿ≥ÿØ ÿØŸàÿ±Ÿáÿß ÿßŸÑÿ±ŸäÿßÿØŸä ŸÅŸä ÿ™ÿπÿ≤Ÿäÿ≤ ÿßŸÑÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ© ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜŸäÿ© Ÿàÿ®ŸÜÿßÿ° ÿ¨ÿ≥Ÿàÿ± ÿßŸÑÿ™ÿ∂ÿßŸÖŸÜ ŸÖÿπ ÿßŸÑÿ¥ÿπŸàÿ® ÿßŸÑŸÖÿ™ÿ∂ÿ±ÿ±ÿ© ŸÅŸä ŸÖÿÆÿ™ŸÑŸÅ ÿ£ŸÜÿ≠ÿßÿ° ÿßŸÑÿπÿßŸÑŸÖ.

**CRITICAL REQUIREMENTS:**
- The news article MUST be about the headline provided: "{claim_text}"
- You MUST use ALL the information from the Fact-check Analysis provided below
- The Fact-check Analysis contains the actual facts and details - extract them and write the news article based on them
- Do NOT invent or create unrelated news - only use information from the analysis
- Follow the exact structure shown in the example above
- First paragraph: Start directly with the event from the headline (who, what, when, where, participants) using details from the analysis
- Second paragraph: Discuss the details, quantities, beneficiaries, or specific information from the analysis
- Third paragraph: Additional context about significance, continuation, or broader implications from the analysis
- Write as a direct news report, NOT as verification or fact-check
- AVOID any mention of "verification", "fact-check", "results", "ÿ™ÿ≠ŸÇŸÇ", "ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑÿ™ÿ≠ŸÇŸÇ"
- Use the analysis data to inform your reporting, but present it as breaking news
- The article MUST be relevant to the headline: "{claim_text}"
- Adapt the structure to the target language ({lang.upper()}) while maintaining the same meaning
"""
    else:
        user_message = f"""
**PROVIDED DATA:**
Headline: {claim_text}
Fact-check Analysis: {talk}

**AVAILABLE SOURCES:**
{sources_context}

**EXAMPLE FORMAT FOR UNCERTAIN NEWS (ARABIC):**
ÿ™ÿØÿßŸàŸÑÿ™ ŸÖŸÜÿµÿßÿ™ ÿßŸÑÿ™ŸàÿßÿµŸÑ ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπŸä ŸÖÿ≤ÿßÿπŸÖ ÿ™ŸÅŸäÿØ ÿ®ÿ£ŸÜ [ÿßŸÑÿßÿØÿπÿßÿ°]ÿå ÿ∫Ÿäÿ± ÿ£ŸÜ ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑÿ™ÿ≠ŸÇŸÇ ÿ£ÿ∏Ÿáÿ±ÿ™ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿßÿØÿπÿßÿ° ŸÑÿß ŸäŸÖŸÉŸÜ ÿ™ÿ£ŸÉŸäÿØŸá.

Ÿàÿ®ÿ≠ÿ≥ÿ® ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖÿ™ÿßÿ≠ÿ©ÿå [ÿ¥ÿ±ÿ≠ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖÿ™ÿßÿ≠ÿ© ŸàÿßŸÑÿ≥ÿ®ÿ® ŸÅŸä ÿπÿØŸÖ ÿßŸÑÿ™ÿ£ŸÉŸäÿØ]. [ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ™ÿßÿ±ŸäÿÆŸäÿ© ÿ£Ÿà ÿ≥ŸäÿßŸÇ ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸÖÿ™ÿßÿ≠ÿßŸã].

Ÿàÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿ∞ŸÑŸÉÿå Ÿäÿ™ÿ®ŸäŸëŸÜ ÿ£ŸÜ ÿßŸÑÿßÿØÿπÿßÿ° ÿßŸÑŸÖÿ™ÿØÿßŸàŸÑ ŸäŸÅÿ™ŸÇÿ± ÿ•ŸÑŸâ ÿ£Ÿä ÿ£ÿ≥ÿßÿ≥ ŸÖŸÜ ÿßŸÑÿ£ÿØŸÑÿ© ÿßŸÑŸÖŸàÿ´ŸàŸÇÿ©ÿå ŸàŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÖÿµÿßÿØÿ± ÿ™ÿØÿπŸÖ ÿµÿ≠ÿ™Ÿá.

**INSTRUCTIONS:**
- Follow the exact structure shown in the example above
- Use the analysis data to explain why the claim cannot be confirmed
- Include historical context or relevant background when available
- End with the conclusion that the claim lacks reliable evidence
- Adapt the structure to the target language ({lang.upper()}) while maintaining the same meaning
"""
    
    try:
        print("üì∞ Generating news article...")
        
        response = await client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": FACT_CHECK_NEWS_PROMPT},
                {"role": "user", "content": user_message}
            ],
            temperature=0.1,  # Very low temperature for factual, measured content
            max_tokens=400,   # Allow enough tokens for 150-250 words
            top_p=0.9,        # Focus on most likely responses
            frequency_penalty=0.1,  # Slight penalty to avoid repetition
            presence_penalty=0.1    # Encourage diverse vocabulary
        )
        
        article = response.choices[0].message.content.strip()
        print("‚úÖ News article generated successfully")
        return article
        
    except Exception as e:
        print(f"‚ùå Error generating news article: {e}")
        error_messages = {
            "ar": "ÿπÿ∞ÿ±ÿßŸãÿå ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑŸÖŸÇÿßŸÑ ÿßŸÑÿ•ÿÆÿ®ÿßÿ±Ÿä. Ÿäÿ±ÿ¨Ÿâ ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ.",
            "en": "Sorry, an error occurred while writing the news article. Please try again.",
            "fr": "D√©sol√©, une erreur s'est produite lors de la r√©daction de l'article de presse. Veuillez r√©essayer.",
            "es": "Lo siento, ocurri√≥ un error al escribir el art√≠culo de noticias. Por favor, int√©ntalo de nuevo.",
        }
        return error_messages.get(lang, error_messages["en"])

async def generate_x_tweet_async(claim_text: str, case: str, talk: str, sources: List[Dict], lang: str = "ar", client: AsyncOpenAI = None) -> str:
    """
    Generate a professional X (Twitter) tweet based on fact-check results
    Optimized for X platform with proper formatting and engagement
    """
    
    # X/Twitter specific prompt
    X_TWEET_PROMPT = f"""
You are a professional social media journalist and X (Twitter) content creator with expertise in:

**X PLATFORM EXPERTISE:**
1. **Social Media Journalist**: Create engaging, accurate news content for X
2. **Viral Content Creator**: Understand what drives engagement on X
3. **Fact-Checking Specialist**: Present verified information clearly
4. **Crisis Communication**: Handle sensitive information responsibly
5. **Community Manager**: Engage audiences while maintaining credibility
6. **Digital Storyteller**: Tell compelling stories in limited characters
7. **Breaking News Reporter**: Handle urgent, time-sensitive information
8. **Public Interest Communicator**: Serve public interest on social media

**X PLATFORM REQUIREMENTS:**
- Maximum 280 characters (strict limit)
- Use hashtags strategically (2-3 relevant hashtags)
- Include emojis appropriately for engagement
- Write for mobile-first audience
- Use clear, concise language
- Include call-to-action when appropriate
- Maintain professional credibility
- Respect X community guidelines

**TWEET STRUCTURE FOR FACT-CHECKING:**
1. **Hook**: Attention-grabbing opening
2. **Fact**: Clear statement of the fact-check result
3. **Context**: Brief explanation or key detail
4. **Hashtags**: Relevant, trending hashtags
5. **Emojis**: Strategic use for engagement and clarity

**LANGUAGE POLICY:**
- Write ENTIRELY in {lang.upper()} language
- Use professional but engaging tone
- Adapt to social media communication style
- Maintain journalistic credibility
- Use appropriate emojis for the language/culture

**ENGAGEMENT STRATEGY:**
- Start with compelling hook
- Use numbers/statistics when available
- Include relevant hashtags
- Use emojis strategically
- End with clear conclusion or call-to-action
- Maintain professional credibility

**RESPONSE FORMAT:**
Generate a single, professional X tweet (max 280 characters) that:
- Clearly states the fact-check result
- Engages the audience appropriately
- Maintains journalistic credibility
- Uses relevant hashtags and emojis
- Respects X platform guidelines
"""

    # Prepare context based on fact-check result (only True or Uncertain)
    if case.lower() in {"ÿ≠ŸÇŸäŸÇŸä", "true", "vrai", "verdadero", "pravda"}:
        result_emoji = "‚úÖ"
        result_text = "ÿ≠ŸÇŸäŸÇŸä" if lang == "ar" else "TRUE"
        tone = "confirming"
    else:  # uncertain
        result_emoji = "‚ö†Ô∏è"
        result_text = "ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ" if lang == "ar" else "UNCERTAIN"
        tone = "uncertain"

    # Create the user message
    user_message = f"""
**FACT-CHECK RESULT:**
Claim: {claim_text}
Result: {case} ({result_text})
Analysis: {talk}

**SOURCES:**
{len(sources)} sources available

**INSTRUCTIONS:**
Create a professional X tweet that:
1. Clearly communicates the fact-check result
2. Engages the audience appropriately
3. Uses relevant hashtags and emojis
4. Maintains journalistic credibility
5. Respects X platform guidelines
6. Stays within 280 character limit

**TONE:** {tone}
**LANGUAGE:** {lang.upper()}
**PLATFORM:** X (Twitter)
**CHARACTER LIMIT:** 280 characters maximum
"""

    try:
        print("üê¶ Generating X tweet...")
        
        response = await client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": X_TWEET_PROMPT},
                {"role": "user", "content": user_message}
            ],
            temperature=0.3,  # Balanced creativity and accuracy
            max_tokens=100,   # Optimized for tweet length (280 chars max)
            top_p=0.9,
            frequency_penalty=0.1,
            presence_penalty=0.1
        )
        
        tweet = response.choices[0].message.content.strip()
        
        # Ensure tweet is within character limit
        if len(tweet) > 280:
            tweet = tweet[:277] + "..."
        
        print("‚úÖ X tweet generated successfully")
        return tweet
        
    except Exception as e:
        print(f"‚ùå Error generating X tweet: {e}")
        error_messages = {
            "ar": "‚ö†Ô∏è ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑÿ™ÿ∫ÿ±ŸäÿØÿ©. Ÿäÿ±ÿ¨Ÿâ ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ.",
            "en": "‚ö†Ô∏è An error occurred while generating the tweet. Please try again.",
            "fr": "‚ö†Ô∏è Une erreur s'est produite lors de la g√©n√©ration du tweet. Veuillez r√©essayer.",
            "es": "‚ö†Ô∏è Ocurri√≥ un error al generar el tweet. Por favor, int√©ntalo de nuevo.",
        }
        return error_messages.get(lang, error_messages["en"])

SERPAPI_KEY = os.getenv("SERPAPI_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")
SERPAPI_HL = os.getenv("SERPAPI_HL", "ar")
SERPAPI_GL = os.getenv("SERPAPI_GL", "")
NEWS_AGENCIES = [d.strip() for d in os.getenv("NEWS_AGENCIES", "aljazeera.net,una-oic.org,bbc.com").split(",") if d.strip()]

if not SERPAPI_KEY or not OPENAI_API_KEY:
    raise RuntimeError("‚ö†Ô∏è ÿ±ÿ¨ÿßÿ°Ÿã ÿ∂ÿπ SERPAPI_KEY Ÿà OPENAI_API_KEY ŸÅŸä .env")

# Create async OpenAI client
async_client = AsyncOpenAI(api_key=OPENAI_API_KEY)

async def _lang_hint_from_claim_async(text: str) -> str:
    try:
        resp = await async_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "Detect the input language and return ONLY its ISO 639-1 code (like ar, en, fr, es, de)."},
                {"role": "user", "content": text.strip()},
            ],
            temperature=0.0,
            max_tokens=5
        )
        lang = (resp.choices[0].message.content or "").strip().lower()
        if len(lang) == 2:
            return lang
    except Exception:
        pass

    # fallback
    ar_count = sum(1 for ch in text if '\u0600' <= ch <= '\u06FF')
    ratio = ar_count / max(1, len(text))
    return "ar" if ratio >= 0.15 else "en"

async def is_news_content_async(text: str) -> tuple[bool, str]:
    """
    Validate if the input text is news/journalistic content SPECIFICALLY about Gaza, Palestine, or OIC (async version).
    Returns (is_valid, reason) tuple.
    If not news-related OR not about Gaza/Palestine/OIC, returns (False, reason in Arabic).
    """
    try:
        validation_prompt = """You are a news content validator for a SPECIALIZED FACT-CHECKING API focused ONLY on Gaza, Palestine, and the Organization of Islamic Cooperation (OIC).

üéØ **STRICT SCOPE LIMITATION:**
This API ONLY accepts news claims/statements that are DIRECTLY related to:
1. **Gaza** (ÿ∫ÿ≤ÿ©) - Any news, events, statements, or claims about Gaza Strip
2. **Palestine** (ŸÅŸÑÿ≥ÿ∑ŸäŸÜ) - Any news, events, statements, or claims about Palestine, Palestinian territories, Palestinian people, Palestinian-Israeli conflict, Palestinian Authority, Palestinian government, Palestinian cities (Ramallah, Nablus, Hebron, Bethlehem, etc.), Palestinian refugees, Palestinian cause
3. **Organization of Islamic Cooperation (OIC)** (ŸÖŸÜÿ∏ŸÖÿ© ÿßŸÑÿ™ÿπÿßŸàŸÜ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸä) - Any news, events, statements, or claims about OIC, its member states' actions related to Palestine/Gaza, OIC summits, OIC statements, OIC resolutions, OIC humanitarian aid

‚ö†Ô∏è KEY DISTINCTION: Accept STATEMENTS/CLAIMS about events, NOT personal questions asking for opinions or information.

‚úÖ ACCEPT (News Claims/Statements ABOUT GAZA/PALESTINE/OIC ONLY):
- STATEMENTS about Gaza events (e.g., "ŸÇÿµŸÅ ÿ•ÿ≥ÿ±ÿßÿ¶ŸäŸÑŸä ÿπŸÑŸâ ÿ∫ÿ≤ÿ©" = YES)
- STATEMENTS about Palestine events (e.g., "ÿßÿ¨ÿ™ŸÖÿßÿπ ŸÅŸä ÿ±ÿßŸÖ ÿßŸÑŸÑŸá" = YES)
- STATEMENTS about Palestinian-Israeli conflict (e.g., "ÿßÿ¥ÿ™ÿ®ÿßŸÉÿßÿ™ ŸÅŸä ÿßŸÑÿ∂ŸÅÿ© ÿßŸÑÿ∫ÿ±ÿ®Ÿäÿ©" = YES)
- STATEMENTS about OIC actions/resolutions regarding Palestine/Gaza (e.g., "ÿ•ÿπŸÑÿßŸÜ ŸÖŸÜÿ∏ŸÖÿ© ÿßŸÑÿ™ÿπÿßŸàŸÜ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸä" = YES)
- CLAIMS about Palestinian Authority, Palestinian government, Palestinian cities
- NEWS HEADLINES about Gaza, Palestine, or OIC-related Palestine news
- Declarative sentences about events, people, places IN Gaza, Palestine, or related to OIC-Palestine
- ANY CLAIM that can be fact-checked AND is about Gaza/Palestine/OIC-Palestine

‚ùå REJECT (Content OUTSIDE Gaza/Palestine/OIC scope):
- ANY claim NOT about Gaza, Palestine, or OIC-Palestine (e.g., "ÿ≤ŸÑÿ≤ÿßŸÑ ŸÅŸä ÿ™ÿ±ŸÉŸäÿß" = NO - wrong location)
- News about other countries unless it's OIC action related to Palestine/Gaza
- General world news not related to Palestine/Gaza/OIC
- Sports news unless it's about Palestinian teams or Gaza
- Celebrity news unless it's about Palestinian celebrities or Gaza-related
- QUESTIONS asking for opinions (e.g., "ŸÖÿß ÿ±ÿ£ŸäŸÉ ŸÅŸä ÿßŸÑŸàÿ∂ÿπÿü" = NO)
- QUESTIONS asking for information (e.g., "ŸÉŸäŸÅ ÿßŸÑÿ∑ŸÇÿ≥ ÿßŸÑŸäŸàŸÖÿü" = NO)
- How-to guides, recipes (e.g., "ÿ∑ÿ±ŸäŸÇÿ© ÿπŸÖŸÑ ÿßŸÑŸÖÿ≠ÿ¥Ÿä" = NO)
- Casual conversations, greetings ("ŸÖÿ±ÿ≠ÿ®ÿßÿå ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü" = NO)
- Educational tutorials ("ŸÉŸäŸÅ ÿ£ÿ™ÿπŸÑŸÖ ÿßŸÑÿ®ÿ±ŸÖÿ¨ÿ©" = NO)
- Personal questions without specific claim
- Philosophical discussions without Gaza/Palestine/OIC news context
- General knowledge questions
- Requests for advice or tips

üîë THE KEY TESTS:
1. Is it a STATEMENT/CLAIM about something that happened or will happen?
2. Is it DIRECTLY related to Gaza, Palestine, or OIC-Palestine actions?
- If YES to both ‚Üí ACCEPT (it can be fact-checked)
- If NO to either ‚Üí REJECT (not in scope)

EXAMPLES - ACCEPT ‚úÖ:
- "ŸÇÿµŸÅ ÿ•ÿ≥ÿ±ÿßÿ¶ŸäŸÑŸä ÿπŸÑŸâ ÿ∫ÿ≤ÿ©" ‚Üí YES (Gaza-related claim)
- "ÿßÿ¨ÿ™ŸÖÿßÿπ ŸÅŸä ÿ±ÿßŸÖ ÿßŸÑŸÑŸá" ‚Üí YES (Palestine-related claim)
- "ŸÖŸÜÿ∏ŸÖÿ© ÿßŸÑÿ™ÿπÿßŸàŸÜ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸä ÿ™ÿØŸäŸÜ ÿßŸÑÿπÿØŸàÿßŸÜ ÿπŸÑŸâ ÿ∫ÿ≤ÿ©" ‚Üí YES (OIC-Palestine related)
- "ÿßÿ≥ÿ™ÿ¥ŸáÿßÿØ ŸÅŸÑÿ≥ÿ∑ŸäŸÜŸä ŸÅŸä ÿßŸÑÿ∂ŸÅÿ© ÿßŸÑÿ∫ÿ±ÿ®Ÿäÿ©" ‚Üí YES (Palestine-related claim)
- "ŸÖÿ≥ÿßÿπÿØÿßÿ™ ÿ•ŸÜÿ≥ÿßŸÜŸäÿ© ÿ•ŸÑŸâ ÿ∫ÿ≤ÿ©" ‚Üí YES (Gaza-related claim)
- "ŸÇÿ±ÿßÿ± ŸÖŸÜÿ∏ŸÖÿ© ÿßŸÑÿ™ÿπÿßŸàŸÜ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸä ÿ®ÿ¥ÿ£ŸÜ ŸÅŸÑÿ≥ÿ∑ŸäŸÜ" ‚Üí YES (OIC-Palestine related)
- "ŸÖÿ∏ÿßŸáÿ±ÿßÿ™ ŸÜÿµÿ±ÿ© ŸÑÿ∫ÿ≤ÿ©" ‚Üí YES (Gaza-related claim)

EXAMPLES - REJECT ‚ùå:
- "ÿ≤ŸÑÿ≤ÿßŸÑ Ÿäÿ∂ÿ±ÿ® ÿ™ÿ±ŸÉŸäÿß" ‚Üí NO (not Gaza/Palestine/OIC-related)
- "ŸÖŸÇÿ™ŸÑ ÿ™ÿ±ÿßŸÖÿ®" ‚Üí NO (not Gaza/Palestine/OIC-related)
- "ÿ•ŸÜÿ¥ÿßÿ° ŸÇÿ∑ÿßÿ± Ÿäÿ±ÿ®ÿ∑ ÿßŸÑÿØŸàÿ≠ÿ© ÿ®ÿßŸÑÿ±Ÿäÿßÿ∂" ‚Üí NO (not Gaza/Palestine/OIC-related)
- "ÿ≠ÿ±ŸäŸÇ ŸÅŸä ŸÖÿ®ŸÜŸâ ÿ®ÿ±ÿ¨ ÿÆŸÑŸäŸÅÿ©" ‚Üí NO (not Gaza/Palestine/OIC-related)
- "ŸÅŸàÿ≤ ÿßŸÑŸáŸÑÿßŸÑ ÿ®ÿßŸÑÿØŸàÿ±Ÿä" ‚Üí NO (not Gaza/Palestine/OIC-related)
- "ŸÖÿß ÿ±ÿ£ŸäŸÉ ŸÅŸä ÿßŸÑÿ∑ŸÇÿ≥ ÿßŸÑŸäŸàŸÖÿü" ‚Üí NO (question asking for opinion)
- "ŸÉŸäŸÅ ÿßŸÑÿ∑ŸÇÿ≥ ÿßŸÑŸäŸàŸÖÿü" ‚Üí NO (question asking for information)
- "ŸáŸÑ ÿ™ÿπÿ™ŸÇÿØ ÿ£ŸÜ ÿßŸÑÿßŸÇÿ™ÿµÿßÿØ ÿ≥Ÿäÿ™ÿ≠ÿ≥ŸÜÿü" ‚Üí NO (opinion question, not Gaza/Palestine/OIC-specific)
- "ÿ∑ÿ±ŸäŸÇÿ© ÿπŸÖŸÑ ÿßŸÑŸÖÿ≠ÿ¥Ÿä" ‚Üí NO (how-to/recipe)
- "ŸÉŸäŸÅ ÿ£ÿ™ÿπŸÑŸÖ ÿßŸÑÿ®ÿ±ŸÖÿ¨ÿ©" ‚Üí NO (educational question)
- "ŸÖÿ±ÿ≠ÿ®ÿßÿå ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü" ‚Üí NO (casual greeting)
- "ŸÖÿß ŸáŸä ÿ£ŸÅÿ∂ŸÑ ÿ∑ÿ±ŸäŸÇÿ© ŸÑŸÑÿ≥ŸÅÿ±ÿü" ‚Üí NO (advice question)

‚ö†Ô∏è CRITICAL: 
1. A CLAIM/STATEMENT can be fact-checked. A QUESTION asking for opinion/info cannot.
2. The claim MUST be about Gaza, Palestine, or OIC-Palestine actions. Other topics are OUT OF SCOPE.

Respond with ONLY one word: "yes" if it's a news claim/statement ABOUT GAZA/PALESTINE/OIC, "no" if it's not.
Then on a new line, provide a CLEAR and DETAILED explanation in Arabic explaining why the content is rejected.

**IMPORTANT FOR REJECTION MESSAGES:**
- If the content is OUTSIDE Gaza/Palestine/OIC scope: Explain clearly that this API is specialized ONLY for Gaza, Palestine, and OIC-related news. Mention what the content is about and why it doesn't fit.
- If it's a question: Explain that only news claims/statements are accepted, not questions.
- Be specific and helpful - tell the user exactly what is wrong and what they should send instead.

Example rejection messages:
- "Ÿáÿ∞ÿß ÿßŸÑÿÆÿ®ÿ± Ÿäÿ™ÿπŸÑŸÇ ÿ®ÿ™ÿ±ŸÉŸäÿßÿå ÿ®ŸäŸÜŸÖÿß Ÿáÿ∞ÿß ÿßŸÑŸÜÿ∏ÿßŸÖ ŸÖÿ™ÿÆÿµÿµ ŸÅŸÇÿ∑ ŸÅŸä ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿ∫ÿ≤ÿ© ŸàŸÅŸÑÿ≥ÿ∑ŸäŸÜ ŸàŸÖŸÜÿ∏ŸÖÿ© ÿßŸÑÿ™ÿπÿßŸàŸÜ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸä. Ÿäÿ±ÿ¨Ÿâ ÿ•ÿ±ÿ≥ÿßŸÑ ÿÆÿ®ÿ± ŸÖÿ™ÿπŸÑŸÇ ÿ®Ÿáÿ∞ÿß ÿßŸÑÿ≥ŸäÿßŸÇ ŸÅŸÇÿ∑."
- "ÿßŸÑŸÜÿµ ÿßŸÑŸÖŸÇÿØŸÖ ÿ≥ÿ§ÿßŸÑ ŸàŸÑŸäÿ≥ ÿÆÿ®ÿ±ÿßŸã ÿ•ÿÆÿ®ÿßÿ±ŸäÿßŸã. Ÿäÿ±ÿ¨Ÿâ ÿ•ÿ±ÿ≥ÿßŸÑ ÿÆÿ®ÿ± ÿ£Ÿà ÿßÿØÿπÿßÿ° ŸÖÿ™ÿπŸÑŸÇ ÿ®ÿ∫ÿ≤ÿ© ÿ£Ÿà ŸÅŸÑÿ≥ÿ∑ŸäŸÜ ÿ£Ÿà ŸÖŸÜÿ∏ŸÖÿ© ÿßŸÑÿ™ÿπÿßŸàŸÜ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸä."
- "Ÿáÿ∞ÿß ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ŸÑÿß Ÿäÿ™ÿπŸÑŸÇ ÿ®ÿ∫ÿ≤ÿ© ÿ£Ÿà ŸÅŸÑÿ≥ÿ∑ŸäŸÜ ÿ£Ÿà ŸÖŸÜÿ∏ŸÖÿ© ÿßŸÑÿ™ÿπÿßŸàŸÜ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸä. Ÿäÿ±ÿ¨Ÿâ ÿ•ÿ±ÿ≥ÿßŸÑ ÿÆÿ®ÿ± ŸÖÿ™ÿπŸÑŸÇ ÿ®Ÿáÿ∞ÿß ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ™ÿÆÿµÿµ ŸÅŸÇÿ∑."""

        resp = await async_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": validation_prompt},
                {"role": "user", "content": text.strip()},
            ],
            temperature=0.1,
            max_tokens=200  # ÿ≤ŸäÿßÿØÿ© ÿßŸÑŸÄ tokens ŸÑŸÑÿ≥ŸÖÿßÿ≠ ÿ®ÿ¥ÿ±ÿ≠ ÿ£Ÿàÿ∂ÿ≠
        )
        
        answer = (resp.choices[0].message.content or "").strip().lower()
        lines = answer.split('\n', 1)
        is_valid = lines[0].strip() == "yes"
        reason = lines[1].strip() if len(lines) > 1 else ""
        
        if not is_valid:
            # ÿ•ÿ∞ÿß ŸÑŸÖ ŸäŸÉŸÜ ŸáŸÜÿßŸÉ ÿ≥ÿ®ÿ® Ÿàÿßÿ∂ÿ≠ÿå ŸÜÿπÿ∑Ÿä ÿ±ÿ≥ÿßŸÑÿ© ÿ™Ÿàÿ∂Ÿäÿ≠Ÿäÿ© ŸÖŸÅÿµŸÑÿ©
            if not reason or len(reason.strip()) < 20:
                reason = f"""‚ö†Ô∏è Ÿáÿ∞ÿß ÿßŸÑŸÜÿ∏ÿßŸÖ ŸÖÿ™ÿÆÿµÿµ ŸÅŸÇÿ∑ ŸÅŸä ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ŸÄ:
‚Ä¢ ÿ∫ÿ≤ÿ© (ŸÇÿ∑ÿßÿπ ÿ∫ÿ≤ÿ©)
‚Ä¢ ŸÅŸÑÿ≥ÿ∑ŸäŸÜ (ÿßŸÑÿ£ÿ±ÿßÿ∂Ÿä ÿßŸÑŸÅŸÑÿ≥ÿ∑ŸäŸÜŸäÿ©ÿå ÿßŸÑÿ¥ÿπÿ® ÿßŸÑŸÅŸÑÿ≥ÿ∑ŸäŸÜŸäÿå ÿßŸÑÿ≥ŸÑÿ∑ÿ© ÿßŸÑŸÅŸÑÿ≥ÿ∑ŸäŸÜŸäÿ©)
‚Ä¢ ŸÖŸÜÿ∏ŸÖÿ© ÿßŸÑÿ™ÿπÿßŸàŸÜ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸä (ÿÆÿßÿµÿ© ŸÅŸäŸÖÿß Ÿäÿ™ÿπŸÑŸÇ ÿ®ŸÅŸÑÿ≥ÿ∑ŸäŸÜ Ÿàÿ∫ÿ≤ÿ©)

ÿßŸÑŸÜÿµ ÿßŸÑŸÖŸÇÿØŸÖ ŸÑÿß Ÿäÿ™ÿπŸÑŸÇ ÿ®Ÿáÿ∞ÿß ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ™ÿÆÿµÿµ. Ÿäÿ±ÿ¨Ÿâ ÿ•ÿ±ÿ≥ÿßŸÑ ÿÆÿ®ÿ± ÿ£Ÿà ÿßÿØÿπÿßÿ° ŸÖÿ™ÿπŸÑŸÇ ÿ®ÿ∫ÿ≤ÿ© ÿ£Ÿà ŸÅŸÑÿ≥ÿ∑ŸäŸÜ ÿ£Ÿà ŸÖŸÜÿ∏ŸÖÿ© ÿßŸÑÿ™ÿπÿßŸàŸÜ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸä ŸÅŸÇÿ∑."""
            return (False, reason)
        return (True, "")
        
    except Exception as e:
        # On error, allow through but log it
        print(f"‚ö†Ô∏è Error validating news content: {e}")
        return (True, "")  # Allow through on error to avoid blocking valid requests

async def _fetch_serp_async(session: aiohttp.ClientSession, query: str, extra: Dict | None = None, num: int = 10) -> List[Dict]:
    url = "https://serpapi.com/search.json"
    params = {
        "q": query,
        "api_key": SERPAPI_KEY,
        "hl": SERPAPI_HL,
        "gl": SERPAPI_GL,
        "num": num
    }
    if extra:
        params.update(extra)
    try:
        print(f"üîç Fetching: {query}")
        async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=20)) as response:
            response.raise_for_status()
            data = await response.json()
            results = []
            for it in data.get("organic_results", []):
                results.append({
                    "title": it.get("title") or "",
                    "snippet": it.get("snippet") or (it.get("snippet_highlighted_words", [""]) or [""])[0],
                    "link": it.get("link") or it.get("displayed_link") or "",
                })
            print(f"‚úÖ Found {len(results)} results for query: {query}")
            return [r for r in results if r["title"] or r["snippet"] or r["link"]]
    except Exception as e:
        print(f"‚ùå Error fetching from SerpAPI: {e}")
        return []

FACT_PROMPT_SYSTEM = (
    "You are a rigorous fact-checking assistant. Use ONLY the sources provided below.\n"
    "- You can ONLY return TWO possible verdicts: True OR Uncertain.\n"
    "- If the claim is supported by credible sources with clear evidence ‚Üí verdict: True\n"
    "- If evidence is insufficient, conflicting, unclear, or off-topic ‚Üí verdict: Uncertain\n"
    "- IMPORTANT: There is NO 'False' option. If you cannot confirm something as True, mark it as Uncertain.\n"
    "- Prefer official catalogs and reputable agencies over blogs or social posts.\n"
    "- Match the claim's date/place/magnitude when relevant; do not infer beyond the given sources.\n\n"

    "LANGUAGE POLICY:\n"
    "- You MUST respond **entirely** in the language specified by LANG_HINT.\n"
    "- Do NOT switch to another language or translate.\n"
    "- Examples:\n"
    "   ‚Ä¢ If LANG_HINT = 'fr' ‚Üí respond fully in French.\n"
    "   ‚Ä¢ If LANG_HINT = 'ar' ‚Üí respond fully in Arabic.\n"
    "   ‚Ä¢ If LANG_HINT = 'en' ‚Üí respond fully in English.\n"
    "   ‚Ä¢ If LANG_HINT = 'es' ‚Üí respond fully in Spanish.\n"
    "   ‚Ä¢ If LANG_HINT = 'cs' ‚Üí respond fully in Czech.\n\n"

    "FORMAT RULES:\n"
    "‚Ä¢ You MUST write all free-text fields strictly in LANG_HINT language.\n"
    "‚Ä¢ JSON keys must remain EXACTLY as: \"ÿßŸÑÿ≠ÿßŸÑÿ©\", \"talk\", \"sources\" (do not translate keys).\n"
    "‚Ä¢ The value of \"ÿßŸÑÿ≠ÿßŸÑÿ©\" must be ONLY one of these two options (localized):\n"
    "   - Arabic: ÿ≠ŸÇŸäŸÇŸä / ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ (ONLY these two options)\n"
    "   - English: True / Uncertain (ONLY these two options)\n"
    "   - French: Vrai / Incertain (ONLY these two options)\n"
    "   - Spanish: Verdadero / Incierto (ONLY these two options)\n"
    "   - Czech: Pravda / Nejist√© (ONLY these two options)\n"
    "‚Ä¢ NEVER use: False, Faux, Falso, Nepravda, ŸÉÿßÿ∞ÿ® - these are NOT valid options!\n"

    "RESPONSE FORMAT (JSON ONLY ‚Äî no extra text):\n"
    "{\n"
    '  \"ÿßŸÑÿ≠ÿßŸÑÿ©\": \"<Localized verdict: True OR Uncertain ONLY>\",\n'
    '  \"talk\": \"<Explanation paragraph ~350 words in LANG_HINT>\",\n'
    '  \"sources\": [ {\"title\": \"<title>\", \"url\": \"<url>\"}, ... ]\n'
    "}\n\n"

    "SOURCES RULES:\n"
    "1) Include ONLY sources that DIRECTLY support or relate to the claim.\n"
    "2) Do NOT include unrelated sources, even if they mention similar topics.\n"
    "3) If a source title/content is NOT relevant to the claim ‚Üí DO NOT include it.\n"
    "4) Maximum 10 sources (prioritize the most relevant and credible ones).\n"
    "5) Remove duplicate URLs - include each source only once.\n"
    "6) Each source must have both title AND url.\n\n"

    "FINAL RULES:\n"
    "1) Output STRICTLY valid JSON (UTF-8). No extra commentary before or after.\n"
    "2) If the claim is Uncertain ‚Üí keep 'sources' as an empty array [].\n"
    "3) If the claim is True ‚Üí include ONLY RELEVANT confirming sources (max 10).\n"
    "4) Do not fabricate URLs or titles; use only provided sources.\n"
    "5) REMEMBER: You can ONLY return True or Uncertain. There is NO False option.\n"
    "6) ONLY include sources that are DIRECTLY related to the specific claim.\n"
)


async def check_fact_simple_async(claim_text: str, k_sources: int = 5, generate_news: bool = False, preserve_sources: bool = False, generate_tweet: bool = False) -> dict:
    try:
        # ÿ™ÿ±ÿ¨ŸÖÿ© ÿßŸÑŸÖÿ±ÿßÿ¨ÿπ ÿßŸÑÿ≤ŸÖŸÜŸäÿ© ŸÅŸä ÿßŸÑŸÜÿµ
        processed_claim = translate_date_references(claim_text)
        print(f"üß† Fact-checking: {processed_claim}")
        
        # Create aiohttp session for parallel HTTP requests
        async with aiohttp.ClientSession() as session:
            # Run language detection and searches in parallel for maximum speed
            lang_task = _lang_hint_from_claim_async(processed_claim)
            
            # Prepare all search queries (start immediately without waiting for language)
            search_tasks = []
            
            # Add news agency searches
            for domain in NEWS_AGENCIES:
                search_tasks.append(
                    _fetch_serp_async(session, f"{processed_claim} site:{domain}", extra=None, num=2)
                )
            
            # Add general Google search
            search_tasks.append(
                _fetch_serp_async(session, processed_claim, extra=None, num=k_sources)
            )
            
            # Execute language detection and all searches in parallel
            print(f"üöÄ Running language detection + {len(search_tasks)} parallel search queries...")
            all_results = await asyncio.gather(lang_task, *search_tasks)
            
            # Extract language and search results
            lang = all_results[0]
            search_results = all_results[1:]
            
            # Combine all results and remove duplicates based on URL
            results = []
            seen_urls = set()
            for result_list in search_results:
                for result in result_list:
                    url = result.get("link", "")
                    # Only add if URL is not empty and not seen before
                    if url and url not in seen_urls:
                        results.append(result)
                        seen_urls.add(url)

        print(f"üîé Total combined results: {len(results)}")

        if not results:
            no_results_by_lang = {
                "ar": "ŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿ≠ÿ´.",
                "en": "No search results were found.",
                "fr": "Aucun r√©sultat de recherche trouv√©.",
                "es": "No se encontraron resultados de b√∫squeda.",
                "cs": "Nebyly nalezeny ≈æ√°dn√© v√Ωsledky vyhled√°v√°n√≠.",
                "de": "Es wurden keine Suchergebnisse gefunden.",
                "tr": "Arama sonu√ßlarƒ± bulunamadƒ±.",
                "ru": "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.",
            }
            return {"case": "ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ", "talk": no_results_by_lang.get(lang, no_results_by_lang["en"]), "sources": [], "news_article": None}

        def clip(s: str, n: int) -> str:
            return s.strip() if len(s) <= n else s[:n] + "‚Ä¶"

        context = "\n\n---\n\n".join(
            f"ÿπŸÜŸàÿßŸÜ: {clip(r['title'], 100)}\nŸÖŸÑÿÆÿµ: {clip(r['snippet'], 200)}\nÿ±ÿßÿ®ÿ∑: {r['link']}"
            for r in results
        )

        system_prompt = FACT_PROMPT_SYSTEM.replace("LANG_HINT", lang)
        user_msg = f"""
LANG_HINT: {lang}
CURRENT_DATE: {datetime.now().strftime('%Y-%m-%d')}

ÿßŸÑÿßÿØÿπÿßÿ°:
{processed_claim}

ÿßŸÑÿ≥ŸäÿßŸÇ:
{context}
""".strip()

        print("üì§ Sending prompt to OpenAI (fact-checking)")
        resp = await async_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_msg},
            ],
            temperature=0.2,
            max_tokens=800,  # Enough for comprehensive fact-check
            response_format={"type": "json_object"},
        )
        answer = (resp.choices[0].message.content or "").strip()
        
        # Clean up the answer - remove markdown code blocks if present
        if answer.startswith("```"):
            answer = answer.strip("` \n")
            if answer.lower().startswith("json"):
                answer = answer[4:].strip()
        
        # Try to extract JSON if it's wrapped in other text
        json_match = re.search(r'\{[\s\S]*\}', answer)
        if json_match:
            answer = json_match.group(0)
        
        # Parse JSON with error handling
        parsed = None
        try:
            parsed = json.loads(answer)
        except json.JSONDecodeError as e:
            # Downgrade noisy logs; only show if explicitly debugging
            if os.getenv("FACT_DEBUG", "0") == "1":
                print(f"‚ö†Ô∏è JSON parsing error: {e}")
                print(f"üìÑ Response content (first 1000 chars): {answer[:1000]}")
            
            # Strategy 1: Smart extraction and reconstruction
            # Instead of trying to fix malformed JSON, extract and rebuild it properly
            try:
                # Extract case
                case_match = re.search(r'"ÿßŸÑÿ≠ÿßŸÑÿ©"\s*:\s*"([^"]+)"', answer)
                case = case_match.group(1) if case_match else "ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ"
                
                # Extract talk - find everything between "talk": " and "sources"
                talk_start = answer.find('"talk": "')
                talk = ""
                if talk_start != -1:
                    talk_value_start = talk_start + 9
                    # Find where "sources" begins
                    sources_pos = answer.find('",\n  "sources"', talk_value_start)
                    if sources_pos == -1:
                        sources_pos = answer.find('",\n  "sources"', talk_value_start)
                    if sources_pos == -1:
                        sources_pos = answer.find('\n  "sources"', talk_value_start)
                    if sources_pos == -1:
                        sources_pos = answer.find('"sources"', talk_value_start)
                    
                    if sources_pos != -1:
                        # Extract content between "talk": " and "sources"
                        talk_raw = answer[talk_value_start:sources_pos].rstrip()
                        # Remove trailing comma and quote if exists
                        talk_raw = talk_raw.rstrip(',').rstrip()
                        if talk_raw.endswith('"'):
                            talk_raw = talk_raw[:-1]
                        # Clean up escape sequences
                        talk = talk_raw.replace('\\"', '"').replace('\\n', '\n').replace('\\\\', '\\')
                    else:
                        # Fallback: find until end of JSON
                        end_brace = answer.rfind('}', talk_value_start)
                        if end_brace != -1:
                            talk_raw = answer[talk_value_start:end_brace].rstrip().rstrip(',').rstrip()
                            if talk_raw.endswith('"'):
                                talk_raw = talk_raw[:-1]
                            talk = talk_raw.replace('\\"', '"').replace('\\n', '\n').replace('\\\\', '\\')
                
                if not talk:
                    talk = "ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖÿ™ÿßÿ≠ÿ©."
                
                # Extract sources array - more robust pattern
                sources = []
                sources_match = re.search(r'"sources"\s*:\s*\[(.*?)\]', answer, re.DOTALL)
                if sources_match:
                    sources_str = sources_match.group(1)
                    # Try multiple patterns to extract sources
                    # Pattern 1: Standard format with title and url
                    source_pattern = r'\{\s*"title"\s*:\s*"([^"]+)"\s*,\s*"url"\s*:\s*"([^"]+)"'
                    for src_match in re.finditer(source_pattern, sources_str):
                        sources.append({
                            "title": src_match.group(1),
                            "url": src_match.group(2)
                        })
                    
                    # Pattern 2: If no sources found, try with different spacing
                    if not sources:
                        source_pattern2 = r'"title"\s*:\s*"([^"]+)"\s*[,\s]+\s*"url"\s*:\s*"([^"]+)"'
                        for src_match in re.finditer(source_pattern2, sources_str):
                            sources.append({
                                "title": src_match.group(1),
                                "url": src_match.group(2)
                            })
                    
                    # Pattern 3: Try to extract from parsed JSON if available
                    if not sources and parsed:
                        # If parsed is a dict, try to get sources directly
                        if isinstance(parsed, dict):
                            sources_from_parsed = parsed.get("sources", [])
                            if sources_from_parsed and isinstance(sources_from_parsed, list):
                                sources = sources_from_parsed
                
                # If still no sources and case is "ÿ≠ŸÇŸäŸÇŸä", use original search results
                if not sources and case.lower() in {"ÿ≠ŸÇŸäŸÇŸä", "true", "vrai", "verdadero", "pravda"}:
                    # Use original search results as sources
                    sources = [{"title": r.get("title", ""), "url": r.get("link", ""), "snippet": r.get("snippet", "")} for r in results[:5]]
                    print(f"üìö Using {len(sources)} original search results as sources")
                
                # Rebuild valid JSON dict (no need to parse, just use the dict)
                rebuilt_json = {
                    "ÿßŸÑÿ≠ÿßŸÑÿ©": case,
                    "talk": talk,
                    "sources": sources
                }
                
                # Use the rebuilt dict directly
                parsed = rebuilt_json
                print("‚úÖ Rebuilt JSON from extracted fields")
                
            except Exception as rebuild_error:
                print(f"‚ö†Ô∏è Rebuild failed: {rebuild_error}")
                parsed = None
            
            # Strategy 2: Use regex extraction if JSON parsing still fails
            if parsed is None:
                # Extract fields using regex - more robust for malformed JSON
                try:
                    # Extract case
                    case_match = re.search(r'"ÿßŸÑÿ≠ÿßŸÑÿ©"\s*:\s*"([^"]+)"', answer)
                    case = case_match.group(1) if case_match else "ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ"
                    
                    # Extract talk - handle multi-line strings more carefully
                    # Find the talk field and extract everything until "sources" or end
                    talk_start = answer.find('"talk": "')
                    talk = ""
                    if talk_start != -1:
                        talk_value_start = talk_start + 9
                        # Find where talk should end (before "sources" or closing brace)
                        sources_pos = answer.find(',\n  "sources"', talk_value_start)
                        if sources_pos == -1:
                            sources_pos = answer.find(',\n  "sources"', talk_value_start)
                        if sources_pos == -1:
                            sources_pos = answer.find('"sources"', talk_value_start)
                        if sources_pos == -1:
                            # Find the closing brace before "sources" array
                            end_brace = answer.rfind('}', talk_value_start)
                            if end_brace != -1:
                                # Look backwards for the end of talk string
                                before_sources = answer[talk_value_start:end_brace]
                                # Find the last quote before sources or end
                                last_quote = before_sources.rfind('"')
                                if last_quote != -1:
                                    talk = before_sources[:last_quote]
                                else:
                                    talk = before_sources.rstrip().rstrip(',').rstrip()
                        else:
                            talk = answer[talk_value_start:sources_pos].rstrip().rstrip(',').rstrip()
                            # Remove trailing quote if exists
                            if talk.endswith('"'):
                                talk = talk[:-1]
                        # Clean up escape sequences
                        talk = talk.replace('\\"', '"').replace('\\n', '\n').replace('\\\\', '\\').strip()
                    else:
                        talk = "ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖÿ™ÿßÿ≠ÿ©."
                    
                    # Extract sources array - more robust pattern
                    sources = []
                    sources_match = re.search(r'"sources"\s*:\s*\[(.*?)\]', answer, re.DOTALL)
                    if sources_match:
                        sources_str = sources_match.group(1)
                        # Try multiple patterns to extract sources
                        source_pattern = r'\{\s*"title"\s*:\s*"([^"]+)"\s*,\s*"url"\s*:\s*"([^"]+)"'
                        for src_match in re.finditer(source_pattern, sources_str):
                            sources.append({
                                "title": src_match.group(1),
                                "url": src_match.group(2)
                            })
                        
                        # If no sources found, try with different spacing
                        if not sources:
                            source_pattern2 = r'"title"\s*:\s*"([^"]+)"\s*[,\s]+\s*"url"\s*:\s*"([^"]+)"'
                            for src_match in re.finditer(source_pattern2, sources_str):
                                sources.append({
                                    "title": src_match.group(1),
                                    "url": src_match.group(2)
                                })
                    
                    # If still no sources and case is "ÿ≠ŸÇŸäŸÇŸä", use original search results
                    if not sources and case.lower() in {"ÿ≠ŸÇŸäŸÇŸä", "true", "vrai", "verdadero", "pravda"}:
                        # Use original search results as sources
                        sources = [{"title": r.get("title", ""), "url": r.get("link", ""), "snippet": r.get("snippet", "")} for r in results[:5]]
                        print(f"üìö Using {len(sources)} original search results as sources")
                    
                    parsed = {
                        "ÿßŸÑÿ≠ÿßŸÑÿ©": case,
                        "talk": talk,
                        "sources": sources
                    }
                    print("‚úÖ Extracted JSON using regex fallback")
                except Exception as parse_error:
                    print(f"‚ö†Ô∏è Regex extraction also failed: {parse_error}")
                    parsed = None
            
            # Strategy 3: Final fallback - return uncertain result
            if parsed is None:
                print(f"‚ùå Failed to parse JSON with all strategies")
                # Return uncertain result as fallback
                return {
                    "case": "ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ",
                    "talk": "ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ŸÖÿπÿßŸÑÿ¨ÿ© ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑÿ™ÿ≠ŸÇŸÇ. Ÿäÿ±ÿ¨Ÿâ ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ.",
                    "sources": [],
                    "news_article": None,
                    "x_tweet": None
                }

        case = parsed.get("ÿßŸÑÿ≠ÿßŸÑÿ©", "ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ")
        talk = parsed.get("talk", "")
        sources = parsed.get("sources", [])
        
        # Remove duplicates and irrelevant sources
        if sources:
            unique_sources = []
            seen_source_urls = set()
            
            # Extract key words from claim (ignore common stop words)
            stop_words = {'ŸÅŸä', 'ŸÖŸÜ', 'ÿ•ŸÑŸâ', 'ÿπŸÑŸâ', 'ÿπŸÜ', 'ŸÖÿπ', 'Ÿáÿ∞ÿß', 'Ÿáÿ∞Ÿá', 'ÿ∞ŸÑŸÉ', 'ÿßŸÑÿ™Ÿä', 'ÿßŸÑÿ∞Ÿä', 
                         'Ÿà', 'ÿ£Ÿà', 'ŸÑŸÉŸÜ', 'ŸÅ', 'ÿ®', 'ŸÉ', 'ŸÑ', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 
                         'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}
            claim_words = set(word.lower() for word in processed_claim.split() if word.lower() not in stop_words and len(word) > 2)
            
            for source in sources:
                source_url = source.get("url", "")
                source_title = source.get("title", "").lower()
                source_snippet = source.get("snippet", "").lower()
                
                # Skip if URL is empty or already seen
                if not source_url or source_url in seen_source_urls:
                    continue
                
                # Check if source is relevant to the claim
                # More strict relevance check: title + snippet should contain meaningful key words
                title_words = set(word.lower() for word in source_title.split() if word.lower() not in stop_words and len(word) > 2)
                snippet_words = set(word.lower() for word in source_snippet.split() if word.lower() not in stop_words and len(word) > 2)
                all_source_words = title_words | snippet_words
                
                # Calculate relevance score
                if claim_words and all_source_words:
                    common_words = claim_words & all_source_words
                    relevance_ratio = len(common_words) / len(claim_words) if claim_words else 0
                    
                    # More lenient threshold to ensure we get enough sources
                    # Require at least 20% overlap OR at least 1-2 key words in common
                    min_common = max(1, int(len(claim_words) * 0.2))
                    
                    # Accept if relevance is reasonable (20% or has at least min_common words)
                    if len(common_words) >= min_common or relevance_ratio >= 0.2:
                        unique_sources.append(source)
                        seen_source_urls.add(source_url)
                        if os.getenv("FACT_DEBUG", "0") == "1":
                            print(f"‚úì Relevant source: {source_title[:50]}... (score: {relevance_ratio:.2f}, common: {len(common_words)})")
                    else:
                        if os.getenv("FACT_DEBUG", "0") == "1":
                            print(f"‚úó Filtered out: {source_title[:50]}... (score: {relevance_ratio:.2f}, common: {len(common_words)})")
                elif len(source_title) > 0:
                    # If claim has no meaningful words, just check if source has title
                    unique_sources.append(source)
                    seen_source_urls.add(source_url)
            
            sources = unique_sources
            
            # Ensure we have at least 3 sources if available from original results
            # If we filtered too aggressively and have < 3 sources, add more from results
            if len(sources) < 3 and len(results) > 0:
                print(f"‚ö†Ô∏è Only {len(sources)} sources after filtering, adding more from search results...")
                # Add sources from original results that haven't been added yet
                for r in results[:10]:
                    url = r.get("link", "")
                    if url and url not in seen_source_urls:
                        sources.append({
                            "title": r.get("title", ""),
                            "url": url,
                            "snippet": r.get("snippet", "")
                        })
                        seen_source_urls.add(url)
                        if len(sources) >= 5:  # Target at least 5 sources
                            break
                print(f"üìö Now have {len(sources)} sources after adding from search results")
            
            # Limit sources to top 10 to avoid overwhelming response
            if len(sources) > 10:
                sources = sources[:10]
                print(f"üìö Limited sources to top 10 (from {len(unique_sources)})")
        
        # Ensure sources are returned for "ÿ≠ŸÇŸäŸÇŸä" cases
        # If no sources found and case is "ÿ≠ŸÇŸäŸÇŸä", use original search results
        if not sources and case.lower() in {"ÿ≠ŸÇŸäŸÇŸä", "true", "vrai", "verdadero", "pravda"}:
            sources = [{"title": r.get("title", ""), "url": r.get("link", ""), "snippet": r.get("snippet", "")} for r in results[:5]]
            print(f"üìö Using {len(sources)} original search results as sources for verified claim")

        uncertain_terms = {
            "ar": {"ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ"},
            "en": {"uncertain"},
            "fr": {"incertain"},
            "es": {"incierto"},
            "cs": {"nejist√©", "nejiste", "nejist√°"},
            "de": {"unsicher"},
            "tr": {"belirsiz"},
            "ru": {"–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ", "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ", "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π"},
        }
        lowered = case.strip().lower()
        is_uncertain = lowered in {t for s in uncertain_terms.values() for t in s}
        
        # Prepare parallel tasks for news and tweet generation
        generation_tasks = []
        news_article = ""
        x_tweet = ""
        
        if generate_news:
            print("üì∞ Generating professional news article as requested...")
            generation_tasks.append(
                generate_professional_news_article_from_analysis_async(processed_claim, case, talk, results, lang, async_client)
            )
        
        if generate_tweet:
            print("üê¶ Generating X tweet as requested...")
            generation_tasks.append(
                generate_x_tweet_async(processed_claim, case, talk, results, lang, async_client)
            )
        
        # Execute generation tasks in parallel if any
        if generation_tasks:
            print(f"üöÄ Running {len(generation_tasks)} parallel generation tasks...")
            generation_results = await asyncio.gather(*generation_tasks)
            
            # Assign results based on what was requested
            result_idx = 0
            if generate_news:
                news_article = generation_results[result_idx]
                result_idx += 1
            if generate_tweet:
                x_tweet = generation_results[result_idx]
        
        # Clear sources for uncertain results unless explicitly requested to preserve them
        # But if preserve_sources is true, use the original search results instead of AI sources
        if is_uncertain:
            if preserve_sources:
                # Use original search results when preserving sources (already deduplicated)
                sources = [{"title": r.get("title", ""), "url": r.get("link", ""), "snippet": r.get("snippet", "")} for r in results]
            else:
                # Clear sources as per original logic
                sources = []

        return {
            "case": case, 
            "talk": talk, 
            "sources": sources,
            "news_article": news_article if generate_news else None,
            "x_tweet": x_tweet if generate_tweet else None
        }

    except Exception as e:
        print("‚ùå Error:", traceback.format_exc())
        error_by_lang = {
            "ar": "‚ö†Ô∏è ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ÿßŸÑÿ™ÿ≠ŸÇŸÇ.",
            "en": "‚ö†Ô∏è An error occurred during fact-checking.",
            "fr": "‚ö†Ô∏è Une erreur s'est produite lors de la v√©rification des faits.",
            "es": "‚ö†Ô∏è Se produjo un error durante la verificaci√≥n de hechos.",
            "cs": "‚ö†Ô∏è Bƒõhem ovƒõ≈ôov√°n√≠ fakt≈Ø do≈°lo k chybƒõ.",
            "de": "‚ö†Ô∏è Bei der Faktenpr√ºfung ist ein Fehler aufgetreten.",
            "tr": "‚ö†Ô∏è Doƒürulama sƒ±rasƒ±nda bir hata olu≈ütu.",
            "ru": "‚ö†Ô∏è –í–æ –≤—Ä–µ–º—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞.",
        }
        try:
            lang = await _lang_hint_from_claim_async(processed_claim if 'processed_claim' in locals() else claim_text)
        except Exception:
            lang = "en"
        return {"case": "ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ", "talk": error_by_lang.get(lang, error_by_lang["en"]), "sources": [], "news_article": None}


# Keep synchronous version for backward compatibility - it will call async version internally
def check_fact_simple(claim_text: str, k_sources: int = 5, generate_news: bool = False, preserve_sources: bool = False, generate_tweet: bool = False) -> dict:
    """Synchronous wrapper for async fact-checking"""
    return asyncio.run(check_fact_simple_async(claim_text, k_sources, generate_news, preserve_sources, generate_tweet))

